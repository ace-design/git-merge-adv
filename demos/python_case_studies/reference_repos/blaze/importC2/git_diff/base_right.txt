diff --git a/python_case_studies/reference_repos/blaze/importC2/base.py b/python_case_studies/reference_repos/blaze/importC2/right.py
index 6ec5f70..eb7ffee 100644
--- a/python_case_studies/reference_repos/blaze/importC2/base.py
+++ b/python_case_studies/reference_repos/blaze/importC2/right.py
@@ -1,7 +1,7 @@
 from datetime import timedelta
 from operator import methodcaller
 import itertools
-import tempfile,math
+import tempfile
 
 import pytest
 
@@ -19,6 +19,7 @@ from odo import odo, resource, drop, discover
 from blaze import symbol, compute, concat, by, join, sin, cos, radians, atan2
 from odo.utils import tmpfile
 from blaze import sqrt, transform, Data
+from blaze.interactive import iscorescalar
 from blaze.utils import example, normalize
 
 
@@ -258,14 +259,14 @@ def test_postgres_isnan(sql_with_float):
     data = (1.0,), (float('nan'),)
     table = odo(data, sql_with_float)
     sym = symbol('s', discover(data))
-    assert odo(compute(sym.isnan(), table), list) == [(False,), (True,)]
+    assert compute(sym.isnan(), table, return_type=list) == [(False,), (True,)]
 
 
 def test_insert_from_subselect(sql_with_float):
     data = pd.DataFrame([{'c': 2.0}, {'c': 1.0}])
     tbl = odo(data, sql_with_float)
     s = symbol('s', discover(data))
-    odo(compute(s[s.c.isin((1.0, 2.0))].sort(), tbl), sql_with_float),
+    odo(compute(s[s.c.isin((1.0, 2.0))].sort(), tbl, return_type='native'), sql_with_float),
     tm.assert_frame_equal(
         odo(sql_with_float, pd.DataFrame).iloc[2:].reset_index(drop=True),
         pd.DataFrame([{'c': 1.0}, {'c': 2.0}]),
@@ -282,10 +283,7 @@ def test_concat(sql_two_tables):
     t = symbol('t', discover(t_data))
     u = symbol('u', discover(u_data))
     tm.assert_frame_equal(
-        odo(
-            compute(concat(t, u).sort('a'), {t: t_table, u: u_table}),
-            pd.DataFrame,
-        ),
+        compute(concat(t, u).sort('a'), {t: t_table, u: u_table}, return_type=pd.DataFrame),
         pd.DataFrame(np.arange(10), columns=['a']),
     )
 
@@ -303,7 +301,7 @@ def test_concat_invalid_axis(sql_two_tables):
     u = symbol('u', '5 * 1 * int32')
 
     with pytest.raises(ValueError) as e:
-        compute(concat(t, u, axis=1), {t: t_table, u: u_table})
+        compute(concat(t, u, axis=1), {t: t_table, u: u_table}, return_type='native')
 
     # Preserve the suggestion to use merge.
     assert "'merge'" in str(e.value)
@@ -314,13 +312,13 @@ def test_timedelta_arith(sql_with_dts):
     dates = pd.Series(pd.date_range('2014-01-01', '2014-02-01'))
     sym = symbol('s', discover(dates))
     assert (
-        odo(compute(sym + delta, sql_with_dts), pd.Series) == dates + delta
+        compute(sym + delta, sql_with_dts, return_type=pd.Series) == dates + delta
     ).all()
     assert (
-        odo(compute(sym - delta, sql_with_dts), pd.Series) == dates - delta
+        compute(sym - delta, sql_with_dts, return_type=pd.Series) == dates - delta
     ).all()
     assert (
-        odo(compute(sym - (sym - delta), sql_with_dts), pd.Series) ==
+        compute(sym - (sym - delta), sql_with_dts, return_type=pd.Series) ==
         dates - (dates - delta)
     ).all()
 
@@ -334,7 +332,7 @@ def test_timedelta_stat_reduction(sql_with_timedeltas, func):
     expected = timedelta(
         seconds=getattr(deltas.astype('int64') / 1e9, func)(ddof=expr.unbiased)
     )
-    assert odo(compute(expr, sql_with_timedeltas), timedelta) == expected
+    assert compute(expr, sql_with_timedeltas, return_type=timedelta) == expected
 
 
 def test_coerce_bool_and_sum(sql):
@@ -342,13 +340,13 @@ def test_coerce_bool_and_sum(sql):
     t = symbol(n, discover(sql))
     expr = (t.B > 1.0).coerce(to='int32').sum()
     result = compute(expr, sql).scalar()
-    expected = odo(compute(t.B, sql), pd.Series).gt(1).sum()
+    expected = compute(t.B, sql, return_type=pd.Series).gt(1).sum()
     assert result == expected
 
 
 def test_distinct_on(sql):
     t = symbol('t', discover(sql))
-    computation = compute(t[['A', 'B']].sort('A').distinct('A'), sql)
+    computation = compute(t[['A', 'B']].sort('A').distinct('A'), sql, return_type='native')
     assert normalize(str(computation)) == normalize("""
     SELECT DISTINCT ON (anon_1."A") anon_1."A", anon_1."B"
     FROM (SELECT {tbl}."A" AS "A", {tbl}."B" AS "B"
@@ -360,7 +358,7 @@ def test_distinct_on(sql):
 def test_auto_join_field(orders):
     t = symbol('t', discover(orders))
     expr = t.product_id.color
-    result = compute(expr, orders)
+    result = compute(expr, orders, return_type='native')
     expected = """SELECT
         products.color
     FROM products, orders
@@ -372,7 +370,7 @@ def test_auto_join_field(orders):
 def test_auto_join_projection(orders):
     t = symbol('t', discover(orders))
     expr = t.product_id[['color', 'price']]
-    result = compute(expr, orders)
+    result = compute(expr, orders, return_type='native')
     expected = """SELECT
         products.color,
         products.price
@@ -387,7 +385,7 @@ def test_auto_join_projection(orders):
 def test_foreign_key_reduction(orders, products, func):
     t = symbol('t', discover(orders))
     expr = methodcaller(func)(t.product_id.price)
-    result = compute(expr, orders)
+    result = compute(expr, orders, return_type='native')
     expected = """WITH alias as (select
             products.price as price
         from
@@ -401,7 +399,7 @@ def test_foreign_key_reduction(orders, products, func):
 def test_foreign_key_chain(fkey):
     t = symbol('t', discover(fkey))
     expr = t.sym_id.main.data
-    result = compute(expr, fkey)
+    result = compute(expr, fkey, return_type='native')
     expected = """SELECT
         main.data
     FROM main, fkey, pkey
@@ -416,7 +414,7 @@ def test_foreign_key_chain(fkey):
 def test_foreign_key_group_by(fkey, grouper):
     t = symbol('fkey', discover(fkey))
     expr = by(t.sym_id[grouper], avg_price=t.sym_id.price.mean())
-    result = compute(expr, fkey)
+    result = compute(expr, fkey, return_type='native')
     expected = """SELECT
         pkey.sym,
         avg(pkey.price) AS avg_price
@@ -431,7 +429,7 @@ def test_foreign_key_group_by(fkey, grouper):
 def test_group_by_map(fkey, grouper):
     t = symbol('fkey', discover(fkey))
     expr = by(t[grouper], id_count=t.size.count())
-    result = compute(expr, fkey)
+    result = compute(expr, fkey, return_type='native')
     expected = """SELECT
         fkey.sym_id,
         count(fkey.size) AS id_count
@@ -444,7 +442,7 @@ def test_group_by_map(fkey, grouper):
 def test_foreign_key_isin(fkey):
     t = symbol('fkey', discover(fkey))
     expr = t.sym_id.isin([1, 2])
-    result = compute(expr, fkey)
+    result = compute(expr, fkey, return_type='native')
     expected = """SELECT
         fkey.sym_id IN (%(sym_id_1)s, %(sym_id_2)s) AS anon_1
     FROM fkey
@@ -465,14 +463,14 @@ def test_foreign_key_merge_expression(fkey):
         where
             fkey.sym_id = pkey.id and pkey.main = main.id
     """
-    result = compute(expr, fkey)
+    result = compute(expr, fkey, return_type='native')
     assert normalize(str(result)) == normalize(expected)
 
 
 def test_join_type_promotion(sqla, sqlb):
     t, s = symbol(sqla.name, discover(sqla)), symbol(sqlb.name, discover(sqlb))
     expr = join(t, s, 'B', how='inner')
-    result = set(map(tuple, compute(expr, {t: sqla, s: sqlb}).execute().fetchall()))
+    result = set(map(tuple, compute(expr, {t: sqla, s: sqlb}, return_type='native').execute().fetchall()))
     expected = set([(1, 'a', 'a'), (1, None, 'a')])
     assert result == expected
 
@@ -484,7 +482,7 @@ def test_join_type_promotion(sqla, sqlb):
 def test_shift_on_column(n, column, sql):
     t = symbol('t', discover(sql))
     expr = t[column].shift(n)
-    result = odo(compute(expr, sql), pd.Series)
+    result = compute(expr, sql, return_type=pd.Series)
     expected = odo(sql, pd.DataFrame)[column].shift(n)
     tm.assert_series_equal(result, expected)
 
@@ -493,7 +491,7 @@ def test_shift_on_column(n, column, sql):
 def test_shift_arithmetic(sql, n):
     t = symbol('t', discover(sql))
     expr = t.B - t.B.shift(n)
-    result = odo(compute(expr, sql), pd.Series)
+    result = compute(expr, sql, return_type=pd.Series)
     df = odo(sql, pd.DataFrame)
     expected = df.B - df.B.shift(n)
     tm.assert_series_equal(result, expected)
@@ -524,8 +522,8 @@ def test_dist(nyc):
                     filtered.dropoff_latitude, filtered.dropoff_longitude)
     transformed = transform(filtered, dist=dist)
     assert (
-        odo(compute(transformed.dist.max(), nyc), float) ==
-        odo(compute(transformed.dist, nyc), pd.Series).max()
+        compute(transformed.dist.max(), nyc, return_type=float) ==
+        compute(transformed.dist, nyc, return_type=pd.Series).max()
     )
 
 
@@ -545,7 +543,7 @@ def test_multiple_columns_in_transform(nyc):
     hours = t.trip_time_in_secs.coerce('float64') / 3600.0
     avg_speed_in_mph = t.trip_distance / hours
     d = transform(t, avg_speed_in_mph=avg_speed_in_mph, mycol=avg_speed_in_mph + 1)
-    df = odo(compute(d[d.avg_speed_in_mph <= 200], nyc), pd.DataFrame)
+    df = compute(d[d.avg_speed_in_mph <= 200], nyc, return_type=pd.DataFrame)
     assert not df.empty
 
 
@@ -563,13 +561,31 @@ def test_coerce_on_select(nyc):
         (t.passenger_count < 6)
     ]
     t = transform(t, pass_count=t.passenger_count + 1)
-    result = compute(t.pass_count.coerce('float64'), nyc)
+    result = compute(t.pass_count.coerce('float64'), nyc, return_type='native')
     s = odo(result, pd.Series)
-    expected = odo(compute(t, nyc),
-                   pd.DataFrame).passenger_count.astype('float64') + 1.0
+    expected = compute(t, nyc, return_type=pd.DataFrame) \
+                      .passenger_count.astype('float64') + 1.0
     assert list(s) == list(expected)
 
 
 def test_interactive_len(sql):
     t = Data(sql)
     assert len(t) == int(t.count())
+
+
+def test_core_compute(nyc):
+    t = symbol('t', discover(nyc))
+    assert isinstance(compute(t, nyc, return_type='core'), pd.DataFrame)
+    assert isinstance(compute(t.passenger_count, nyc, return_type='core'), pd.Series)
+    assert iscorescalar(compute(t.passenger_count.mean(), nyc, return_type='core'))
+    assert isinstance(compute(t, nyc, return_type=list), list)
+
+
+def test_sample(sql):
+    t = symbol('t', discover(sql))
+    result = compute(t.sample(n=1), sql)
+    s = odo(result, pd.DataFrame)
+    assert len(s) == 1
+    result2 = compute(t.sample(frac=0.5), sql)
+    s2 = odo(result2, pd.DataFrame)
+    assert len(s) == len(s2)
