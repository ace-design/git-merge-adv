diff --git a/java_case_studies/reference_repos/camus/conflict1/base.java b/java_case_studies/reference_repos/camus/conflict1/left.java
index 46a7f1b..acdc9a1 100644
--- a/java_case_studies/reference_repos/camus/conflict1/base.java
+++ b/java_case_studies/reference_repos/camus/conflict1/left.java
@@ -3,6 +3,7 @@ package com.linkedin.camus.etl.kafka.mapred;
 import com.linkedin.camus.coders.CamusWrapper;
 import com.linkedin.camus.coders.MessageDecoder;
 import com.linkedin.camus.etl.kafka.CamusJob;
+import com.linkedin.camus.etl.kafka.CamusJobTest;
 import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
 import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
 import com.linkedin.camus.etl.kafka.common.EtlKey;
@@ -26,6 +27,7 @@ import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
 import java.util.regex.Pattern;
+
 import kafka.api.PartitionOffsetRequestInfo;
 import kafka.common.ErrorMapping;
 import kafka.common.TopicAndPartition;
@@ -35,6 +37,7 @@ import kafka.javaapi.PartitionMetadata;
 import kafka.javaapi.TopicMetadata;
 import kafka.javaapi.TopicMetadataRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
+
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -76,6 +79,9 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";
   public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";
 
+  public static boolean reportJobFailureDueToOffsetOutOfRange = false;
+  public static boolean useMockRequestForUnitTest = false;
+
   private static Logger log = null;
 
   public EtlInputFormat() {
@@ -328,13 +334,18 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         request.setAvgMsgSize(key.getMessageSize());
       }
 
+      if (useMockRequestForUnitTest) {
+        request = CamusJobTest.mockRequest;
+        context.getConfiguration().setBoolean("KAFKA_MOVE_TO_EARLIEST_OFFSET", false);
+      }
+
       if (request.getEarliestOffset() > request.getOffset() || request.getOffset() > request.getLastOffset()) {
         if (request.getEarliestOffset() > request.getOffset()) {
           log.error("The earliest offset was found to be more than the current offset: " + request);
         } else {
           log.error("The current offset was found to be more than the latest offset: " + request);
         }
-        
+
         boolean move_to_earliest_offset = context.getConfiguration().getBoolean(KAFKA_MOVE_TO_EARLIEST_OFFSET, false);
         boolean offsetUnset = request.getOffset() == EtlRequest.DEFAULT_OFFSET;
         log.info("move_to_earliest: " + move_to_earliest_offset + " offset_unset: " + offsetUnset);
@@ -348,11 +359,12 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
               new EtlKey(request.getTopic(), ((EtlRequest) request).getLeaderId(), request.getPartition(), 0, request
                   .getOffset()));
         } else {
-          log.error("Offset range from kafka metadata is outside the previously persisted offset," +
-                    " please check whether kafka cluster configuration is correct." +
+          log.error("Offset range from kafka metadata is outside the previously persisted offset, " + request + "\n" +
+                    " Topic " + request.getTopic() + " will be skipped.\n" +
+                    " Please check whether kafka cluster configuration is correct." +
                     " You can also specify config parameter: " + KAFKA_MOVE_TO_EARLIEST_OFFSET +
                     " to start processing from earliest kafka metadata offset.");
-          throw new IOException("Offset from kafka metadata is out of range: " + request);
+          reportJobFailureDueToOffsetOutOfRange = true;
         }
       }
       log.info(request);
