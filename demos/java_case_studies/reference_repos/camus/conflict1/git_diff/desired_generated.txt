diff --git a/java_case_studies/reference_repos/camus/conflict1/desired.java b/java_case_studies/demo_results/MethodUnion/camus-conflict1.java
index 805b5a5..d2dbf78 100644
--- a/java_case_studies/reference_repos/camus/conflict1/desired.java
+++ b/java_case_studies/demo_results/MethodUnion/camus-conflict1.java
@@ -1,8 +1,8 @@
 package com.linkedin.camus.etl.kafka.mapred;
-
 import com.linkedin.camus.coders.CamusWrapper;
 import com.linkedin.camus.coders.MessageDecoder;
 import com.linkedin.camus.etl.kafka.CamusJob;
+import com.linkedin.camus.etl.kafka.CamusJobTest;
 import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
 import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
 import com.linkedin.camus.etl.kafka.common.EtlKey;
@@ -10,7 +10,6 @@ import com.linkedin.camus.etl.kafka.common.EtlRequest;
 import com.linkedin.camus.etl.kafka.common.LeaderInfo;
 import com.linkedin.camus.workallocater.CamusRequest;
 import com.linkedin.camus.workallocater.WorkAllocator;
-
 import java.io.IOException;
 import java.net.URI;
 import java.security.InvalidParameterException;
@@ -25,11 +24,10 @@ import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Pattern;
-
 import kafka.api.PartitionOffsetRequestInfo;
 import kafka.common.ErrorMapping;
+import org.apache.hadoop.conf.Configuration;
 import kafka.common.TopicAndPartition;
 import kafka.javaapi.OffsetRequest;
 import kafka.javaapi.OffsetResponse;
@@ -37,8 +35,6 @@ import kafka.javaapi.PartitionMetadata;
 import kafka.javaapi.TopicMetadata;
 import kafka.javaapi.TopicMetadataRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
-
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -54,73 +50,59 @@ import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.log4j.Logger;
 
-import com.linkedin.camus.coders.CamusWrapper;
-import com.linkedin.camus.coders.MessageDecoder;
-import com.linkedin.camus.etl.kafka.CamusJob;
-import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
-import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
-import com.linkedin.camus.etl.kafka.common.EtlKey;
-import com.linkedin.camus.etl.kafka.common.EtlRequest;
-import com.linkedin.camus.etl.kafka.common.LeaderInfo;
-import com.linkedin.camus.workallocater.CamusRequest;
-import com.linkedin.camus.workallocater.WorkAllocator;
-
-
-/**
- * Input format for a Kafka pull job.
- */
-public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
-
-  public static final String KAFKA_BLACKLIST_TOPIC = "kafka.blacklist.topics";
-  public static final String KAFKA_WHITELIST_TOPIC = "kafka.whitelist.topics";
-
-  public static final String KAFKA_MOVE_TO_LAST_OFFSET_LIST = "kafka.move.to.last.offset.list";
-  public static final String KAFKA_MOVE_TO_EARLIEST_OFFSET = "kafka.move.to.earliest.offset";
-
-  public static final String KAFKA_CLIENT_BUFFER_SIZE = "kafka.client.buffer.size";
-  public static final String KAFKA_CLIENT_SO_TIMEOUT = "kafka.client.so.timeout";
-
-  public static final String KAFKA_MAX_PULL_HRS = "kafka.max.pull.hrs";
-  public static final String KAFKA_MAX_PULL_MINUTES_PER_TASK = "kafka.max.pull.minutes.per.task";
-  public static final String KAFKA_MAX_HISTORICAL_DAYS = "kafka.max.historical.days";
-
-  public static final String CAMUS_MESSAGE_DECODER_CLASS = "camus.message.decoder.class";
-  public static final String ETL_IGNORE_SCHEMA_ERRORS = "etl.ignore.schema.errors";
-  public static final String ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST = "etl.audit.ignore.service.topic.list";
-
-  public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";
-  public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";
-
-  public static final int NUM_TRIES_FETCH_FROM_LEADER = 3;
-  public static final int NUM_TRIES_TOPIC_METADATA = 3;
-
-  public static boolean reportJobFailureDueToOffsetOutOfRange = false;
-  public static boolean reportJobFailureUnableToGetOffsetFromKafka = false;
-
-  private static Logger log = null;
-
-  public EtlInputFormat() {
+public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper>{
+
+    public static final String KAFKA_BLACKLIST_TOPIC = "kafka.blacklist.topics";,
+    public static final String KAFKA_WHITELIST_TOPIC = "kafka.whitelist.topics";,
+    public static final String KAFKA_MOVE_TO_LAST_OFFSET_LIST = "kafka.move.to.last.offset.list";,
+    public static final String KAFKA_MOVE_TO_EARLIEST_OFFSET = "kafka.move.to.earliest.offset";,
+    public static final String KAFKA_CLIENT_BUFFER_SIZE = "kafka.client.buffer.size";,
+    public static final String KAFKA_CLIENT_SO_TIMEOUT = "kafka.client.so.timeout";,
+    public static final String KAFKA_MAX_PULL_HRS = "kafka.max.pull.hrs";,
+    public static final String KAFKA_MAX_PULL_MINUTES_PER_TASK = "kafka.max.pull.minutes.per.task";,
+    public static final String KAFKA_MAX_HISTORICAL_DAYS = "kafka.max.historical.days";,
+    public static final String CAMUS_MESSAGE_DECODER_CLASS = "camus.message.decoder.class";,
+    public static final String ETL_IGNORE_SCHEMA_ERRORS = "etl.ignore.schema.errors";,
+    public static final String ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST = "etl.audit.ignore.service.topic.list";,
+    public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";,
+    public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";,
+    public static boolean reportJobFailureDueToOffsetOutOfRange = false;,
+    public static final int NUM_TRIES_FETCH_FROM_LEADER = 3;,
+    public static boolean useMockRequestForUnitTest = false;,
+    public static final int NUM_TRIES_TOPIC_METADATA = 3;,
+    private static Logger log = null;,
+    public static boolean reportJobFailureDueToSkippedMsg = false;,
+
+    public EtlInputFormat() {
     if (log == null)
       log = Logger.getLogger(getClass());
   }
 
-  public static void setLogger(Logger log) {
+    public static void setLogger(Logger log) {
     EtlInputFormat.log = log;
   }
 
-  @Override
+    @Override
   public RecordReader<EtlKey, CamusWrapper> createRecordReader(InputSplit split, TaskAttemptContext context)
       throws IOException, InterruptedException {
     return new EtlRecordReader(this, split, context);
   }
 
-  /**
+    /**
+   * Gets the metadata from Kafka
+   * 
+   * @param context
+   * @return
+   */
+
+    /**
    * Gets the metadata from Kafka
    *
    * @param context
    * @return
    */
-  public List<TopicMetadata> getKafkaMetadata(JobContext context) {
+
+    public List<TopicMetadata> getKafkaMetadata(JobContext context) {
     ArrayList<String> metaRequestTopics = new ArrayList<String>();
     CamusJob.startTiming("kafkaSetupTime");
     String brokerString = CamusJob.getKafkaBrokers(context);
@@ -166,14 +148,32 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return topicMetadataList;
   }
 
-  private SimpleConsumer createBrokerConsumer(JobContext context, String broker) {
+    private SimpleConsumer createConsumer(JobContext context, String broker) {
+    if (!broker.matches(".+:\\d+"))
+      throw new InvalidParameterException("The kakfa broker " + broker + " must follow address:port pattern");
+    String[] hostPort = broker.split(":");
+    SimpleConsumer consumer =
+        new SimpleConsumer(hostPort[0], Integer.valueOf(hostPort[1]), CamusJob.getKafkaTimeoutValue(context),
+            CamusJob.getKafkaBufferSize(context), CamusJob.getKafkaClientName(context));
+    return consumer;
+  }
+
+    /**
+   * Gets the latest offsets and create the requests as needed
+   * 
+   * @param context
+   * @param offsetRequestInfo
+   * @return
+   */
+
+    private SimpleConsumer createBrokerConsumer(JobContext context, String broker) {
     if (!broker.matches(".+:\\d+"))
       throw new InvalidParameterException("The kakfa broker " + broker + " must follow address:port pattern");
     String[] hostPort = broker.split(":");
     return createSimpleConsumer(context, hostPort[0], Integer.valueOf(hostPort[1]));
   }
 
-  public SimpleConsumer createSimpleConsumer(JobContext context, String host, int port) {
+    public SimpleConsumer createSimpleConsumer(JobContext context, String host, int port) {
     SimpleConsumer consumer =
         new SimpleConsumer(host, port,
             CamusJob.getKafkaTimeoutValue(context), CamusJob.getKafkaBufferSize(context),
@@ -181,14 +181,15 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return consumer;
   }
 
-  /**
+    /**
    * Gets the latest offsets and create the requests as needed
    *
    * @param context
    * @param offsetRequestInfo
    * @return
    */
-  public ArrayList<CamusRequest> fetchLatestOffsetAndCreateEtlRequests(JobContext context,
+
+    public ArrayList<CamusRequest> fetchLatestOffsetAndCreateEtlRequests(JobContext context,
       HashMap<LeaderInfo, ArrayList<TopicAndPartition>> offsetRequestInfo) {
     ArrayList<CamusRequest> finalRequests = new ArrayList<CamusRequest>();
     for (LeaderInfo leader : offsetRequestInfo.keySet()) {
@@ -217,7 +218,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
       consumer.close();
       if (earliestOffsetResponse == null) {
         log.warn(generateLogWarnForSkippedTopics(earliestOffsetInfo, consumer));
-        reportJobFailureUnableToGetOffsetFromKafka = true;
+        reportJobFailureDueToSkippedMsg = true;
         continue;
       }
 
@@ -238,7 +239,33 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return finalRequests;
   }
 
-  private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer,
+    public String createTopicRegEx(HashSet<String> topicsSet) {
+    String regex = "";
+    StringBuilder stringbuilder = new StringBuilder();
+    for (String whiteList : topicsSet) {
+      stringbuilder.append(whiteList);
+      stringbuilder.append("|");
+    }
+    regex = "(" + stringbuilder.substring(0, stringbuilder.length() - 1) + ")";
+    Pattern.compile(regex);
+    return regex;
+  }
+
+    public List<TopicMetadata> filterWhitelistTopics(List<TopicMetadata> topicMetadataList,
+      HashSet<String> whiteListTopics) {
+    ArrayList<TopicMetadata> filteredTopics = new ArrayList<TopicMetadata>();
+    String regex = createTopicRegEx(whiteListTopics);
+    for (TopicMetadata topicMetadata : topicMetadataList) {
+      if (Pattern.matches(regex, topicMetadata.topic())) {
+        filteredTopics.add(topicMetadata);
+      } else {
+        log.info("Discarding topic : " + topicMetadata.topic());
+      }
+    }
+    return filteredTopics;
+  }
+
+    private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer,
       Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, JobContext context) {
     for (int i = 0; i < NUM_TRIES_FETCH_FROM_LEADER; i++) {
       try {
@@ -266,43 +293,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return null;
   }
 
-  private String generateLogWarnForSkippedTopics(Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, SimpleConsumer consumer) {
-    StringBuilder sb = new StringBuilder();
-    sb.append("The following topics will be skipped due to failure in fetching latest offsets from leader "
-        + consumer.host() + ":" + consumer.port());
-    for (TopicAndPartition topicAndPartition : offsetInfo.keySet()) {
-      sb.append("  " + topicAndPartition.topic());
-    }
-    return sb.toString();
-  }
-
-  public String createTopicRegEx(HashSet<String> topicsSet) {
-    String regex = "";
-    StringBuilder stringbuilder = new StringBuilder();
-    for (String whiteList : topicsSet) {
-      stringbuilder.append(whiteList);
-      stringbuilder.append("|");
-    }
-    regex = "(" + stringbuilder.substring(0, stringbuilder.length() - 1) + ")";
-    Pattern.compile(regex);
-    return regex;
-  }
-
-  public List<TopicMetadata> filterWhitelistTopics(List<TopicMetadata> topicMetadataList,
-      HashSet<String> whiteListTopics) {
-    ArrayList<TopicMetadata> filteredTopics = new ArrayList<TopicMetadata>();
-    String regex = createTopicRegEx(whiteListTopics);
-    for (TopicMetadata topicMetadata : topicMetadataList) {
-      if (Pattern.matches(regex, topicMetadata.topic())) {
-        filteredTopics.add(topicMetadata);
-      } else {
-        log.info("Discarding topic : " + topicMetadata.topic());
-      }
-    }
-    return filteredTopics;
-  }
-
-  @Override
+    @Override
   public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException {
     CamusJob.startTiming("getSplits");
     ArrayList<CamusRequest> finalRequests;
@@ -455,7 +446,23 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return allocator.allocateWork(finalRequests, context);
   }
 
-  private Set<String> getMoveToLatestTopicsSet(JobContext context) {
+
+    private String generateLogWarnForSkippedTopics(Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, SimpleConsumer consumer) {
+    StringBuilder sb = new StringBuilder();
+    sb.append("The following topics will be skipped due to failure in fetching latest offsets from leader "
+        + consumer.host() + ":" + consumer.port());
+    for (TopicAndPartition topicAndPartition : offsetInfo.keySet()) {
+      sb.append("  " + topicAndPartition.topic());
+    }
+    return sb.toString();
+  }
+
+    @Override
+      public int compare(CamusRequest r1, CamusRequest r2) {
+        return r1.getTopic().compareTo(r2.getTopic());
+      }
+
+    private Set<String> getMoveToLatestTopicsSet(JobContext context) {
     Set<String> topics = new HashSet<String>();
 
     String[] arr = getMoveToLatestTopics(context);
@@ -469,7 +476,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return topics;
   }
 
-  private boolean createMessageDecoder(JobContext context, String topic) {
+    private boolean createMessageDecoder(JobContext context, String topic) {
     try {
       MessageDecoderFactory.createMessageDecoder(context, topic);
       return true;
@@ -479,7 +486,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     }
   }
 
-  private void writePrevious(Collection<EtlKey> missedKeys, JobContext context) throws IOException {
+    private void writePrevious(Collection<EtlKey> missedKeys, JobContext context) throws IOException {
     FileSystem fs = FileSystem.get(context.getConfiguration());
     Path output = FileOutputFormat.getOutputPath(context);
 
@@ -498,7 +505,20 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     writer.close();
   }
 
-  private void writeRequests(List<CamusRequest> requests, JobContext context) throws IOException {
+    public static void setWorkAllocator(JobContext job, Class<WorkAllocator> val) {
+    job.getConfiguration().setClass(CAMUS_WORK_ALLOCATOR_CLASS, val, WorkAllocator.class);
+  }
+
+    public static WorkAllocator getWorkAllocator(JobContext job) {
+    try {
+      return (WorkAllocator) job.getConfiguration()
+          .getClass(CAMUS_WORK_ALLOCATOR_CLASS, Class.forName(CAMUS_WORK_ALLOCATOR_DEFAULT)).newInstance();
+    } catch (Exception e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+    private void writeRequests(List<CamusRequest> requests, JobContext context) throws IOException {
     FileSystem fs = FileSystem.get(context.getConfiguration());
     Path output = FileOutputFormat.getOutputPath(context);
 
@@ -517,7 +537,19 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     writer.close();
   }
 
-  private Map<CamusRequest, EtlKey> getPreviousOffsets(Path[] inputs, JobContext context) throws IOException {
+    public static void setMoveToLatestTopics(JobContext job, String val) {
+    job.getConfiguration().set(KAFKA_MOVE_TO_LAST_OFFSET_LIST, val);
+  }
+
+    public static String[] getMoveToLatestTopics(JobContext job) {
+    return job.getConfiguration().getStrings(KAFKA_MOVE_TO_LAST_OFFSET_LIST);
+  }
+
+    public static void setKafkaClientBufferSize(JobContext job, int val) {
+    job.getConfiguration().setInt(KAFKA_CLIENT_BUFFER_SIZE, val);
+  }
+
+    private Map<CamusRequest, EtlKey> getPreviousOffsets(Path[] inputs, JobContext context) throws IOException {
     Map<CamusRequest, EtlKey> offsetKeysMap = new HashMap<CamusRequest, EtlKey>();
     for (Path input : inputs) {
       FileSystem fs = input.getFileSystem(context.getConfiguration());
@@ -545,137 +577,114 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return offsetKeysMap;
   }
 
-  public static void setWorkAllocator(JobContext job, Class<WorkAllocator> val) {
-    job.getConfiguration().setClass(CAMUS_WORK_ALLOCATOR_CLASS, val, WorkAllocator.class);
-  }
-
-  public static WorkAllocator getWorkAllocator(JobContext job) {
-    try {
-      return (WorkAllocator) job.getConfiguration()
-          .getClass(CAMUS_WORK_ALLOCATOR_CLASS, Class.forName(CAMUS_WORK_ALLOCATOR_DEFAULT)).newInstance();
-    } catch (Exception e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  public static void setMoveToLatestTopics(JobContext job, String val) {
-    job.getConfiguration().set(KAFKA_MOVE_TO_LAST_OFFSET_LIST, val);
-  }
-
-  public static String[] getMoveToLatestTopics(JobContext job) {
-    return job.getConfiguration().getStrings(KAFKA_MOVE_TO_LAST_OFFSET_LIST);
-  }
-
-  public static void setKafkaClientBufferSize(JobContext job, int val) {
-    job.getConfiguration().setInt(KAFKA_CLIENT_BUFFER_SIZE, val);
-  }
-
-  public static int getKafkaClientBufferSize(JobContext job) {
+    public static int getKafkaClientBufferSize(JobContext job) {
     return job.getConfiguration().getInt(KAFKA_CLIENT_BUFFER_SIZE, 2 * 1024 * 1024);
   }
 
-  public static void setKafkaClientTimeout(JobContext job, int val) {
+    public static void setKafkaClientTimeout(JobContext job, int val) {
     job.getConfiguration().setInt(KAFKA_CLIENT_SO_TIMEOUT, val);
   }
 
-  public static int getKafkaClientTimeout(JobContext job) {
+    public static int getKafkaClientTimeout(JobContext job) {
     return job.getConfiguration().getInt(KAFKA_CLIENT_SO_TIMEOUT, 60000);
   }
 
-  public static void setKafkaMaxPullHrs(JobContext job, int val) {
+    public static void setKafkaMaxPullHrs(JobContext job, int val) {
     job.getConfiguration().setInt(KAFKA_MAX_PULL_HRS, val);
   }
 
-  public static int getKafkaMaxPullHrs(JobContext job) {
+    public static int getKafkaMaxPullHrs(JobContext job) {
     return job.getConfiguration().getInt(KAFKA_MAX_PULL_HRS, -1);
   }
 
-  public static void setKafkaMaxPullMinutesPerTask(JobContext job, int val) {
+    public static void setKafkaMaxPullMinutesPerTask(JobContext job, int val) {
     job.getConfiguration().setInt(KAFKA_MAX_PULL_MINUTES_PER_TASK, val);
   }
 
-  public static int getKafkaMaxPullMinutesPerTask(JobContext job) {
+    public static int getKafkaMaxPullMinutesPerTask(JobContext job) {
     return job.getConfiguration().getInt(KAFKA_MAX_PULL_MINUTES_PER_TASK, -1);
   }
 
-  public static void setKafkaMaxHistoricalDays(JobContext job, int val) {
+    public static void setKafkaMaxHistoricalDays(JobContext job, int val) {
     job.getConfiguration().setInt(KAFKA_MAX_HISTORICAL_DAYS, val);
   }
 
-  public static int getKafkaMaxHistoricalDays(JobContext job) {
+    public static int getKafkaMaxHistoricalDays(JobContext job) {
     return job.getConfiguration().getInt(KAFKA_MAX_HISTORICAL_DAYS, -1);
   }
 
-  public static void setKafkaBlacklistTopic(JobContext job, String val) {
+    public static void setKafkaBlacklistTopic(JobContext job, String val) {
     job.getConfiguration().set(KAFKA_BLACKLIST_TOPIC, val);
   }
 
-  public static String[] getKafkaBlacklistTopic(JobContext job) {
-    return getKafkaBlacklistTopic(job.getConfiguration());
-  }
-
-  public static String[] getKafkaBlacklistTopic(Configuration conf) {
-    final String blacklistStr = conf.get(KAFKA_BLACKLIST_TOPIC);
-    if (blacklistStr != null && !blacklistStr.isEmpty()) {
-      return conf.getStrings(KAFKA_BLACKLIST_TOPIC);
-    } else {
-      return new String[] {};
-    }
-  }
-
-  public static void setKafkaWhitelistTopic(JobContext job, String val) {
+    public static void setKafkaWhitelistTopic(JobContext job, String val) {
     job.getConfiguration().set(KAFKA_WHITELIST_TOPIC, val);
   }
 
-  public static String[] getKafkaWhitelistTopic(JobContext job) {
-    return getKafkaWhitelistTopic(job.getConfiguration());
-  }
-
-  public static String[] getKafkaWhitelistTopic(Configuration conf) {
-    final String whitelistStr = conf.get(KAFKA_WHITELIST_TOPIC);
-    if (whitelistStr != null&& !whitelistStr.isEmpty()) {
-      return conf.getStrings(KAFKA_WHITELIST_TOPIC);
-    } else {
-      return new String[] {};
-    }
-  }
-
-  public static void setEtlIgnoreSchemaErrors(JobContext job, boolean val) {
+    public static void setEtlIgnoreSchemaErrors(JobContext job, boolean val) {
     job.getConfiguration().setBoolean(ETL_IGNORE_SCHEMA_ERRORS, val);
   }
 
-  public static boolean getEtlIgnoreSchemaErrors(JobContext job) {
+    public static boolean getEtlIgnoreSchemaErrors(JobContext job) {
     return job.getConfiguration().getBoolean(ETL_IGNORE_SCHEMA_ERRORS, false);
   }
 
-  public static void setEtlAuditIgnoreServiceTopicList(JobContext job, String topics) {
+    public static void setEtlAuditIgnoreServiceTopicList(JobContext job, String topics) {
     job.getConfiguration().set(ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST, topics);
   }
 
-  public static String[] getEtlAuditIgnoreServiceTopicList(JobContext job) {
+    public static String[] getEtlAuditIgnoreServiceTopicList(JobContext job) {
     return job.getConfiguration().getStrings(ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST, "");
   }
 
-  public static void setMessageDecoderClass(JobContext job, Class<MessageDecoder> cls) {
+    public static void setMessageDecoderClass(JobContext job, Class<MessageDecoder> cls) {
     job.getConfiguration().setClass(CAMUS_MESSAGE_DECODER_CLASS, cls, MessageDecoder.class);
   }
 
-  public static Class<MessageDecoder> getMessageDecoderClass(JobContext job) {
+    public static Class<MessageDecoder> getMessageDecoderClass(JobContext job) {
     return (Class<MessageDecoder>) job.getConfiguration().getClass(CAMUS_MESSAGE_DECODER_CLASS,
         KafkaAvroMessageDecoder.class);
   }
 
-  public static Class<MessageDecoder> getMessageDecoderClass(JobContext job, String topicName) {
+    public static Class<MessageDecoder> getMessageDecoderClass(JobContext job, String topicName) {
     Class<MessageDecoder> topicDecoder = (Class<MessageDecoder>) job.getConfiguration().getClass(
             CAMUS_MESSAGE_DECODER_CLASS + "." + topicName, null);
     return topicDecoder == null ? getMessageDecoderClass(job) : topicDecoder;
   }
 
-  private class OffsetFileFilter implements PathFilter {
+    public static String[] getKafkaBlacklistTopic(JobContext job) {
+    return getKafkaBlacklistTopic(job.getConfiguration());
+  }
 
-    @Override
+    private class OffsetFileFilter implements PathFilter{
+
+
+        @Override
     public boolean accept(Path arg0) {
       return arg0.getName().startsWith(EtlMultiOutputFormat.OFFSET_PREFIX);
     }
+
+    }
+    public static String[] getKafkaBlacklistTopic(Configuration conf) {
+    final String blacklistStr = conf.get(KAFKA_BLACKLIST_TOPIC);
+    if (blacklistStr != null && !blacklistStr.isEmpty()) {
+      return conf.getStrings(KAFKA_BLACKLIST_TOPIC);
+    } else {
+      return new String[] {};
+    }
+  }
+
+    public static String[] getKafkaWhitelistTopic(JobContext job) {
+    return getKafkaWhitelistTopic(job.getConfiguration());
   }
-}
+
+    public static String[] getKafkaWhitelistTopic(Configuration conf) {
+    final String whitelistStr = conf.get(KAFKA_WHITELIST_TOPIC);
+    if (whitelistStr != null&& !whitelistStr.isEmpty()) {
+      return conf.getStrings(KAFKA_WHITELIST_TOPIC);
+    } else {
+      return new String[] {};
+    }
+  }
+
+}
\ No newline at end of file
