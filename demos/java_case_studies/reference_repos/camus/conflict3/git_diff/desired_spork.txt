diff --git a/java_case_studies/reference_repos/camus/conflict3/desired.java b/java_case_studies/reference_repos/camus/conflict3/spork_result.java
index e75f0e1..3924734 100644
--- a/java_case_studies/reference_repos/camus/conflict3/desired.java
+++ b/java_case_studies/reference_repos/camus/conflict3/spork_result.java
@@ -1,5 +1,15 @@
 package com.linkedin.camus.etl.kafka.mapred;
 
+import com.linkedin.camus.coders.CamusWrapper;
+import com.linkedin.camus.coders.MessageDecoder;
+import com.linkedin.camus.etl.kafka.CamusJob;
+import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
+import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
+import com.linkedin.camus.etl.kafka.common.EtlKey;
+import com.linkedin.camus.etl.kafka.common.EtlRequest;
+import com.linkedin.camus.etl.kafka.common.LeaderInfo;
+import com.linkedin.camus.workallocater.CamusRequest;
+import com.linkedin.camus.workallocater.WorkAllocator;
 import java.io.IOException;
 import java.net.URI;
 import java.security.InvalidParameterException;
@@ -16,7 +26,6 @@ import java.util.Properties;
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Pattern;
-
 import kafka.api.PartitionOffsetRequestInfo;
 import kafka.common.ErrorMapping;
 import kafka.common.TopicAndPartition;
@@ -26,7 +35,6 @@ import kafka.javaapi.PartitionMetadata;
 import kafka.javaapi.TopicMetadata;
 import kafka.javaapi.TopicMetadataRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
-
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
@@ -43,41 +51,37 @@ import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.log4j.Logger;
 
-import com.linkedin.camus.coders.CamusWrapper;
-import com.linkedin.camus.coders.MessageDecoder;
-import com.linkedin.camus.etl.kafka.CamusJob;
-import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
-import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
-import com.linkedin.camus.etl.kafka.common.EtlKey;
-import com.linkedin.camus.etl.kafka.common.EtlRequest;
-import com.linkedin.camus.etl.kafka.common.LeaderInfo;
-import com.linkedin.camus.workallocater.CamusRequest;
-import com.linkedin.camus.workallocater.WorkAllocator;
-
 
 /**
  * Input format for a Kafka pull job.
  */
 public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
-
   public static final String KAFKA_BLACKLIST_TOPIC = "kafka.blacklist.topics";
+
   public static final String KAFKA_WHITELIST_TOPIC = "kafka.whitelist.topics";
 
   public static final String KAFKA_MOVE_TO_LAST_OFFSET_LIST = "kafka.move.to.last.offset.list";
+
   public static final String KAFKA_MOVE_TO_EARLIEST_OFFSET = "kafka.move.to.earliest.offset";
 
   public static final String KAFKA_CLIENT_BUFFER_SIZE = "kafka.client.buffer.size";
+
   public static final String KAFKA_CLIENT_SO_TIMEOUT = "kafka.client.so.timeout";
 
   public static final String KAFKA_MAX_PULL_HRS = "kafka.max.pull.hrs";
+
   public static final String KAFKA_MAX_PULL_MINUTES_PER_TASK = "kafka.max.pull.minutes.per.task";
+
   public static final String KAFKA_MAX_HISTORICAL_DAYS = "kafka.max.historical.days";
 
   public static final String CAMUS_MESSAGE_DECODER_CLASS = "camus.message.decoder.class";
+
   public static final String ETL_IGNORE_SCHEMA_ERRORS = "etl.ignore.schema.errors";
+
   public static final String ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST = "etl.audit.ignore.service.topic.list";
 
   public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";
+
   public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";
 
   public static final int FETCH_FROM_LEADER_MAX_RETRIES = 3;
@@ -89,8 +93,9 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   private static Logger log = null;
 
   public EtlInputFormat() {
-    if (log == null)
+    if (log == null) {
       log = Logger.getLogger(getClass());
+    }
   }
 
   public static void setLogger(Logger log) {
@@ -105,7 +110,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
 
   /**
    * Gets the metadata from Kafka
-   *
+   * 
    * @param context
    * @return
    */
@@ -113,32 +118,30 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     ArrayList<String> metaRequestTopics = new ArrayList<String>();
     CamusJob.startTiming("kafkaSetupTime");
     String brokerString = CamusJob.getKafkaBrokers(context);
-    if (brokerString.isEmpty())
+    if (brokerString.isEmpty()) {
       throw new InvalidParameterException("kafka.brokers must contain at least one node");
+    }
     List<String> brokers = Arrays.asList(brokerString.split("\\s*,\\s*"));
     Collections.shuffle(brokers);
     boolean fetchMetaDataSucceeded = false;
     int i = 0;
     List<TopicMetadata> topicMetadataList = null;
     Exception savedException = null;
-    while (i < brokers.size() && !fetchMetaDataSucceeded) {
+    while ((i < brokers.size()) && (!fetchMetaDataSucceeded)) {
       SimpleConsumer consumer = createBrokerConsumer(context, brokers.get(i));
-      log.info(String.format("Fetching metadata from broker %s with client id %s for %d topic(s) %s", brokers.get(i),
-          consumer.clientId(), metaRequestTopics.size(), metaRequestTopics));
+      log.info(String.format("Fetching metadata from broker %s with client id %s for %d topic(s) %s", brokers.get(i), consumer.clientId(), metaRequestTopics.size(), metaRequestTopics));
       try {
         for (int iter = 0; iter < NUM_TRIES_TOPIC_METADATA; iter++) {
           try {
             topicMetadataList = consumer.send(new TopicMetadataRequest(metaRequestTopics)).topicsMetadata();
             fetchMetaDataSucceeded = true;
             break;
-          } catch (Exception e) {
+          } catch (java.lang.Exception e) {
             savedException = e;
-            log.warn(
-                     String.format("Fetching topic metadata with client id %s for topics [%s] from broker [%s] failed, iter[%s]",
-                                   consumer.clientId(), metaRequestTopics, brokers.get(i), iter), e);
+            log.warn(String.format("Fetching topic metadata with client id %s for topics [%s] from broker [%s] failed, iter[%s]", consumer.clientId(), metaRequestTopics, brokers.get(i), iter), e);
             try {
-              Thread.sleep((long)(Math.random() * (iter + 1) * 1000));
-            } catch (InterruptedException ex) {
+              Thread.sleep(((long) ((Math.random() * (iter + 1)) * 1000)));
+            } catch (java.lang.InterruptedException ex) {
               log.warn("Caught InterruptedException: " + ex);
             }
           }
@@ -147,7 +150,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         consumer.close();
         i++;
       }
-    }
+    } 
     if (!fetchMetaDataSucceeded) {
       throw new RuntimeException("Failed to obtain metadata!", savedException);
     }
@@ -172,32 +175,26 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
 
   /**
    * Gets the latest offsets and create the requests as needed
-   *
+   * 
    * @param context
    * @param offsetRequestInfo
    * @return
    */
-  public ArrayList<CamusRequest> fetchLatestOffsetAndCreateEtlRequests(JobContext context,
-      HashMap<LeaderInfo, ArrayList<TopicAndPartition>> offsetRequestInfo) {
+  public ArrayList<CamusRequest> fetchLatestOffsetAndCreateEtlRequests(JobContext context, HashMap<LeaderInfo, ArrayList<TopicAndPartition>> offsetRequestInfo) {
     ArrayList<CamusRequest> finalRequests = new ArrayList<CamusRequest>();
     for (LeaderInfo leader : offsetRequestInfo.keySet()) {
       SimpleConsumer consumer = createSimpleConsumer(context, leader.getUri().getHost(), leader.getUri().getPort());
       // Latest Offset
-      PartitionOffsetRequestInfo partitionLatestOffsetRequestInfo =
-          new PartitionOffsetRequestInfo(kafka.api.OffsetRequest.LatestTime(), 1);
+      PartitionOffsetRequestInfo partitionLatestOffsetRequestInfo = new PartitionOffsetRequestInfo(kafka.api.OffsetRequest.LatestTime(), 1);
       // Earliest Offset
-      PartitionOffsetRequestInfo partitionEarliestOffsetRequestInfo =
-          new PartitionOffsetRequestInfo(kafka.api.OffsetRequest.EarliestTime(), 1);
-      Map<TopicAndPartition, PartitionOffsetRequestInfo> latestOffsetInfo =
-          new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
-      Map<TopicAndPartition, PartitionOffsetRequestInfo> earliestOffsetInfo =
-          new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
+      PartitionOffsetRequestInfo partitionEarliestOffsetRequestInfo = new PartitionOffsetRequestInfo(kafka.api.OffsetRequest.EarliestTime(), 1);
+      Map<TopicAndPartition, PartitionOffsetRequestInfo> latestOffsetInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
+      Map<TopicAndPartition, PartitionOffsetRequestInfo> earliestOffsetInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();
       ArrayList<TopicAndPartition> topicAndPartitions = offsetRequestInfo.get(leader);
       for (TopicAndPartition topicAndPartition : topicAndPartitions) {
         latestOffsetInfo.put(topicAndPartition, partitionLatestOffsetRequestInfo);
         earliestOffsetInfo.put(topicAndPartition, partitionEarliestOffsetRequestInfo);
       }
-
       OffsetResponse latestOffsetResponse = getLatestOffsetResponse(consumer, latestOffsetInfo, context);
       OffsetResponse earliestOffsetResponse = null;
       if (latestOffsetResponse != null) {
@@ -209,16 +206,11 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         reportJobFailureDueToSkippedMsg = true;
         continue;
       }
-
       for (TopicAndPartition topicAndPartition : topicAndPartitions) {
         long latestOffset = latestOffsetResponse.offsets(topicAndPartition.topic(), topicAndPartition.partition())[0];
-        long earliestOffset =
-            earliestOffsetResponse.offsets(topicAndPartition.topic(), topicAndPartition.partition())[0];
-
-        //TODO: factor out kafka specific request functionality
-        CamusRequest etlRequest =
-            new EtlRequest(context, topicAndPartition.topic(), Integer.toString(leader.getLeaderId()),
-                topicAndPartition.partition(), leader.getUri());
+        long earliestOffset = earliestOffsetResponse.offsets(topicAndPartition.topic(), topicAndPartition.partition())[0];
+        // TODO: factor out kafka specific request functionality
+        CamusRequest etlRequest = new EtlRequest(context, topicAndPartition.topic(), Integer.toString(leader.getLeaderId()), topicAndPartition.partition(), leader.getUri());
         etlRequest.setLatestOffset(latestOffset);
         etlRequest.setEarliestOffset(earliestOffset);
         finalRequests.add(etlRequest);
@@ -227,27 +219,21 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return finalRequests;
   }
 
-  private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer,
-      Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, JobContext context) {
+  private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer, Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, JobContext context) {
     for (int i = 0; i <= FETCH_FROM_LEADER_MAX_RETRIES; i++) {
       try {
-        OffsetResponse offsetResponse = consumer.getOffsetsBefore(new OffsetRequest(offsetInfo,
-            kafka.api.OffsetRequest.CurrentVersion(), CamusJob.getKafkaClientName(context)));
+        OffsetResponse offsetResponse = consumer.getOffsetsBefore(new OffsetRequest(offsetInfo, kafka.api.OffsetRequest.CurrentVersion(), CamusJob.getKafkaClientName(context)));
         if (offsetResponse.hasError()) {
           throw new RuntimeException("offsetReponse has error.");
         }
         return offsetResponse;
-      } catch (Exception e) {
-        log.warn("Fetching offset from leader " + consumer.host() + ":" + consumer.port()
-            + " has failed " + (i + 1) + " time(s). Reason: "
-            + e.getMessage() + " "
-            + (FETCH_FROM_LEADER_MAX_RETRIES - i) + " retries left.");
+      } catch (java.lang.Exception e) {
+        log.warn(((((((((("Fetching offset from leader " + consumer.host()) + ":") + consumer.port()) + " has failed ") + (i + 1)) + " time(s). Reason: ") + e.getMessage()) + " ") + (FETCH_FROM_LEADER_MAX_RETRIES - i)) + " retries left.");
         if (i < FETCH_FROM_LEADER_MAX_RETRIES) {
           try {
             Thread.sleep(1000 * (i + 1));
-          } catch (InterruptedException e1) {
-            log.error("Caught interrupted exception between retries of getting latest offsets. "
-                + e1.getMessage());
+          } catch (java.lang.InterruptedException e1) {
+            log.error("Caught interrupted exception between retries of getting latest offsets. " + e1.getMessage());
           }
         }
       }
@@ -295,51 +281,39 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException {
     CamusJob.startTiming("getSplits");
     ArrayList<CamusRequest> finalRequests;
-    HashMap<LeaderInfo, ArrayList<TopicAndPartition>> offsetRequestInfo =
-        new HashMap<LeaderInfo, ArrayList<TopicAndPartition>>();
+    HashMap<LeaderInfo, ArrayList<TopicAndPartition>> offsetRequestInfo = new HashMap<LeaderInfo, ArrayList<TopicAndPartition>>();
     try {
-
       // Get Metadata for all topics
       List<TopicMetadata> topicMetadataList = getKafkaMetadata(context);
-
       // Filter any white list topics
       HashSet<String> whiteListTopics = new HashSet<String>(Arrays.asList(getKafkaWhitelistTopic(context)));
       if (!whiteListTopics.isEmpty()) {
         topicMetadataList = filterWhitelistTopics(topicMetadataList, whiteListTopics);
       }
-
       // Filter all blacklist topics
       HashSet<String> blackListTopics = new HashSet<String>(Arrays.asList(getKafkaBlacklistTopic(context)));
       String regex = "";
       if (!blackListTopics.isEmpty()) {
         regex = createTopicRegEx(blackListTopics);
       }
-
       for (TopicMetadata topicMetadata : topicMetadataList) {
         if (Pattern.matches(regex, topicMetadata.topic())) {
           log.info("Discarding topic (blacklisted): " + topicMetadata.topic());
         } else if (!createMessageDecoder(context, topicMetadata.topic())) {
           log.info("Discarding topic (Decoder generation failed) : " + topicMetadata.topic());
         } else if (topicMetadata.errorCode() != ErrorMapping.NoError()) {
-          log.info("Skipping the creation of ETL request for Whole Topic : " + topicMetadata.topic() + " Exception : "
-              + ErrorMapping.exceptionFor(topicMetadata.errorCode()));
+          log.info((("Skipping the creation of ETL request for Whole Topic : " + topicMetadata.topic()) + " Exception : ") + ErrorMapping.exceptionFor(topicMetadata.errorCode()));
         } else {
           for (PartitionMetadata partitionMetadata : topicMetadata.partitionsMetadata()) {
             // We only care about LeaderNotAvailableCode error on partitionMetadata level
             // Error codes such as ReplicaNotAvailableCode should not stop us.
             if (partitionMetadata.errorCode() == ErrorMapping.LeaderNotAvailableCode()) {
-              log.info("Skipping the creation of ETL request for Topic : " + topicMetadata.topic()
-                  + " and Partition : " + partitionMetadata.partitionId() + " Exception : "
-                  + ErrorMapping.exceptionFor(partitionMetadata.errorCode()));
+              log.info((((("Skipping the creation of ETL request for Topic : " + topicMetadata.topic()) + " and Partition : ") + partitionMetadata.partitionId()) + " Exception : ") + ErrorMapping.exceptionFor(partitionMetadata.errorCode()));
             } else {
               if (partitionMetadata.errorCode() != ErrorMapping.NoError()) {
-                log.warn("Receiving non-fatal error code, Continuing the creation of ETL request for Topic : "
-                    + topicMetadata.topic() + " and Partition : " + partitionMetadata.partitionId() + " Exception : "
-                    + ErrorMapping.exceptionFor(partitionMetadata.errorCode()));
+                log.warn((((("Receiving non-fatal error code, Continuing the creation of ETL request for Topic : " + topicMetadata.topic()) + " and Partition : ") + partitionMetadata.partitionId()) + " Exception : ") + ErrorMapping.exceptionFor(partitionMetadata.errorCode()));
               }
-              LeaderInfo leader =
-                  new LeaderInfo(new URI("tcp://" + partitionMetadata.leader().getConnectionString()),
-                      partitionMetadata.leader().id());
+              LeaderInfo leader = new LeaderInfo(new URI("tcp://" + partitionMetadata.leader().getConnectionString()), partitionMetadata.leader().id());
               if (offsetRequestInfo.containsKey(leader)) {
                 ArrayList<TopicAndPartition> topicAndPartitions = offsetRequestInfo.get(leader);
                 topicAndPartitions.add(new TopicAndPartition(topicMetadata.topic(), partitionMetadata.partitionId()));
@@ -349,25 +323,22 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
                 topicAndPartitions.add(new TopicAndPartition(topicMetadata.topic(), partitionMetadata.partitionId()));
                 offsetRequestInfo.put(leader, topicAndPartitions);
               }
-
             }
           }
         }
       }
-    } catch (Exception e) {
+    } catch (java.lang.Exception e) {
       log.error("Unable to pull requests from Kafka brokers. Exiting the program", e);
       throw new IOException("Unable to pull requests from Kafka brokers.", e);
     }
     // Get the latest offsets and generate the EtlRequests
     finalRequests = fetchLatestOffsetAndCreateEtlRequests(context, offsetRequestInfo);
-
     Collections.sort(finalRequests, new Comparator<CamusRequest>() {
       @Override
       public int compare(CamusRequest r1, CamusRequest r2) {
         return r1.getTopic().compareTo(r2.getTopic());
       }
     });
-
     log.info("The requests from kafka metadata are: \n" + finalRequests);
     writeRequests(finalRequests, context);
     Map<CamusRequest, EtlKey> offsetKeys = getPreviousOffsets(FileInputFormat.getInputPaths(context), context);
@@ -375,66 +346,49 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     for (CamusRequest request : finalRequests) {
       if (moveLatest.contains(request.getTopic()) || moveLatest.contains("all")) {
         log.info("Moving to latest for topic: " + request.getTopic());
-        //TODO: factor out kafka specific request functionality
+        // TODO: factor out kafka specific request functionality
         EtlKey oldKey = offsetKeys.get(request);
-        EtlKey newKey =
-            new EtlKey(request.getTopic(), ((EtlRequest) request).getLeaderId(), request.getPartition(), 0,
-                request.getLastOffset());
-
-        if (oldKey != null)
+        EtlKey newKey = new EtlKey(request.getTopic(), ((EtlRequest) (request)).getLeaderId(), request.getPartition(), 0, request.getLastOffset());
+        if (oldKey != null) {
           newKey.setMessageSize(oldKey.getMessageSize());
-
+        }
         offsetKeys.put(request, newKey);
       }
-
       EtlKey key = offsetKeys.get(request);
-
       if (key != null) {
         request.setOffset(key.getOffset());
         request.setAvgMsgSize(key.getMessageSize());
       }
-
-      if (request.getEarliestOffset() > request.getOffset() || request.getOffset() > request.getLastOffset()) {
+      if ((request.getEarliestOffset() > request.getOffset()) || (request.getOffset() > request.getLastOffset())) {
         if (request.getEarliestOffset() > request.getOffset()) {
           log.error("The earliest offset was found to be more than the current offset: " + request);
         } else {
           log.error("The current offset was found to be more than the latest offset: " + request);
         }
-
         boolean move_to_earliest_offset = context.getConfiguration().getBoolean(KAFKA_MOVE_TO_EARLIEST_OFFSET, false);
         boolean offsetUnset = request.getOffset() == EtlRequest.DEFAULT_OFFSET;
-        log.info("move_to_earliest: " + move_to_earliest_offset + " offset_unset: " + offsetUnset);
+        log.info((("move_to_earliest: " + move_to_earliest_offset) + " offset_unset: ") + offsetUnset);
         // When the offset is unset, it means it's a new topic/partition, we also need to consume the earliest offset
         if (move_to_earliest_offset || offsetUnset) {
           log.error("Moving to the earliest offset available");
           request.setOffset(request.getEarliestOffset());
-          offsetKeys.put(
-              request,
-              //TODO: factor out kafka specific request functionality
-              new EtlKey(request.getTopic(), ((EtlRequest) request).getLeaderId(), request.getPartition(), 0, request
-                  .getOffset()));
+          // TODO: factor out kafka specific request functionality
+          offsetKeys.put(request, new EtlKey(request.getTopic(), ((EtlRequest) (request)).getLeaderId(), request.getPartition(), 0, request.getOffset()));
         } else {
-          log.error("Offset range from kafka metadata is outside the previously persisted offset," +
-                    " please check whether kafka cluster configuration is correct." +
-                    " You can also specify config parameter: " + KAFKA_MOVE_TO_EARLIEST_OFFSET +
-                    " to start processing from earliest kafka metadata offset.");
+          log.error((("Offset range from kafka metadata is outside the previously persisted offset," + (" please check whether kafka cluster configuration is correct." + " You can also specify config parameter: ")) + KAFKA_MOVE_TO_EARLIEST_OFFSET) + " to start processing from earliest kafka metadata offset.");
           throw new IOException("Offset from kafka metadata is out of range: " + request);
         }
       }
       log.info(request);
     }
-
     writePrevious(offsetKeys.values(), context);
-
     CamusJob.stopTiming("getSplits");
     CamusJob.startTiming("hadoop");
     CamusJob.setTime("hadoop_start");
-
     WorkAllocator allocator = getWorkAllocator(context);
     Properties props = new Properties();
     props.putAll(context.getConfiguration().getValByRegex(".*"));
     allocator.init(props);
-
     return allocator.allocateWork(finalRequests, context);
   }
 
@@ -484,17 +438,13 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   private void writeRequests(List<CamusRequest> requests, JobContext context) throws IOException {
     FileSystem fs = FileSystem.get(context.getConfiguration());
     Path output = FileOutputFormat.getOutputPath(context);
-
     if (fs.exists(output)) {
       fs.mkdirs(output);
     }
-
     output = new Path(output, EtlMultiOutputFormat.REQUESTS_FILE);
-    SequenceFile.Writer writer =
-        SequenceFile.createWriter(fs, context.getConfiguration(), output, EtlRequest.class, NullWritable.class);
-
+    SequenceFile.Writer writer = SequenceFile.createWriter(fs, context.getConfiguration(), output, EtlRequest.class, NullWritable.class);
     for (CamusRequest r : requests) {
-      //TODO: factor out kafka specific request functionality
+      //TODO: factor out kafka specific request functionality 
       writer.append(r, NullWritable.get());
     }
     writer.close();
@@ -509,7 +459,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         SequenceFile.Reader reader = new SequenceFile.Reader(fs, f.getPath(), context.getConfiguration());
         EtlKey key = new EtlKey();
         while (reader.next(key, NullWritable.get())) {
-          //TODO: factor out kafka specific request functionality
+          //TODO: factor out kafka specific request functionality 
           CamusRequest request = new EtlRequest(context, key.getTopic(), key.getLeaderId(), key.getPartition());
           if (offsetKeysMap.containsKey(request)) {
 
@@ -599,10 +549,10 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
 
   public static String[] getKafkaBlacklistTopic(Configuration conf) {
     final String blacklistStr = conf.get(KAFKA_BLACKLIST_TOPIC);
-    if (blacklistStr != null && !blacklistStr.isEmpty()) {
+    if ((blacklistStr != null) && (!blacklistStr.isEmpty())) {
       return conf.getStrings(KAFKA_BLACKLIST_TOPIC);
     } else {
-      return new String[] {};
+      return new String[]{  };
     }
   }
 
@@ -616,10 +566,10 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
 
   public static String[] getKafkaWhitelistTopic(Configuration conf) {
     final String whitelistStr = conf.get(KAFKA_WHITELIST_TOPIC);
-    if (whitelistStr != null&& !whitelistStr.isEmpty()) {
+    if ((whitelistStr != null) && (!whitelistStr.isEmpty())) {
       return conf.getStrings(KAFKA_WHITELIST_TOPIC);
     } else {
-      return new String[] {};
+      return new String[]{  };
     }
   }
 
@@ -655,7 +605,6 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   }
 
   private class OffsetFileFilter implements PathFilter {
-
     @Override
     public boolean accept(Path arg0) {
       return arg0.getName().startsWith(EtlMultiOutputFormat.OFFSET_PREFIX);
