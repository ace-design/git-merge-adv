diff --git a/java_case_studies/reference_repos/camus/conflict3/desired.java b/java_case_studies/demo_results/MethodUnion/camus-conflict3.java
index e75f0e1..b9f83d4 100644
--- a/java_case_studies/reference_repos/camus/conflict3/desired.java
+++ b/java_case_studies/demo_results/MethodUnion/camus-conflict3.java
@@ -1,5 +1,14 @@
 package com.linkedin.camus.etl.kafka.mapred;
-
+import com.linkedin.camus.coders.CamusWrapper;
+import com.linkedin.camus.coders.MessageDecoder;
+import com.linkedin.camus.etl.kafka.CamusJob;
+import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
+import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
+import com.linkedin.camus.etl.kafka.common.EtlKey;
+import com.linkedin.camus.etl.kafka.common.EtlRequest;
+import com.linkedin.camus.etl.kafka.common.LeaderInfo;
+import com.linkedin.camus.workallocater.CamusRequest;
+import com.linkedin.camus.workallocater.WorkAllocator;
 import java.io.IOException;
 import java.net.URI;
 import java.security.InvalidParameterException;
@@ -14,11 +23,10 @@ import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Pattern;
-
 import kafka.api.PartitionOffsetRequestInfo;
 import kafka.common.ErrorMapping;
+import org.apache.hadoop.conf.Configuration;
 import kafka.common.TopicAndPartition;
 import kafka.javaapi.OffsetRequest;
 import kafka.javaapi.OffsetResponse;
@@ -26,8 +34,6 @@ import kafka.javaapi.PartitionMetadata;
 import kafka.javaapi.TopicMetadata;
 import kafka.javaapi.TopicMetadataRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
-
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -43,73 +49,57 @@ import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.log4j.Logger;
 
-import com.linkedin.camus.coders.CamusWrapper;
-import com.linkedin.camus.coders.MessageDecoder;
-import com.linkedin.camus.etl.kafka.CamusJob;
-import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
-import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
-import com.linkedin.camus.etl.kafka.common.EtlKey;
-import com.linkedin.camus.etl.kafka.common.EtlRequest;
-import com.linkedin.camus.etl.kafka.common.LeaderInfo;
-import com.linkedin.camus.workallocater.CamusRequest;
-import com.linkedin.camus.workallocater.WorkAllocator;
-
-
-/**
- * Input format for a Kafka pull job.
- */
-public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
-
-  public static final String KAFKA_BLACKLIST_TOPIC = "kafka.blacklist.topics";
-  public static final String KAFKA_WHITELIST_TOPIC = "kafka.whitelist.topics";
-
-  public static final String KAFKA_MOVE_TO_LAST_OFFSET_LIST = "kafka.move.to.last.offset.list";
-  public static final String KAFKA_MOVE_TO_EARLIEST_OFFSET = "kafka.move.to.earliest.offset";
-
-  public static final String KAFKA_CLIENT_BUFFER_SIZE = "kafka.client.buffer.size";
-  public static final String KAFKA_CLIENT_SO_TIMEOUT = "kafka.client.so.timeout";
-
-  public static final String KAFKA_MAX_PULL_HRS = "kafka.max.pull.hrs";
-  public static final String KAFKA_MAX_PULL_MINUTES_PER_TASK = "kafka.max.pull.minutes.per.task";
-  public static final String KAFKA_MAX_HISTORICAL_DAYS = "kafka.max.historical.days";
-
-  public static final String CAMUS_MESSAGE_DECODER_CLASS = "camus.message.decoder.class";
-  public static final String ETL_IGNORE_SCHEMA_ERRORS = "etl.ignore.schema.errors";
-  public static final String ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST = "etl.audit.ignore.service.topic.list";
-
-  public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";
-  public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";
-
-  public static final int FETCH_FROM_LEADER_MAX_RETRIES = 3;
-
-  public static boolean reportJobFailureDueToSkippedMsg = false;
-
-  public static final int NUM_TRIES_TOPIC_METADATA = 3;
-
-  private static Logger log = null;
-
-  public EtlInputFormat() {
+public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper>{
+
+    public static final String KAFKA_BLACKLIST_TOPIC = "kafka.blacklist.topics";,
+    public static final String KAFKA_WHITELIST_TOPIC = "kafka.whitelist.topics";,
+    public static final String KAFKA_MOVE_TO_LAST_OFFSET_LIST = "kafka.move.to.last.offset.list";,
+    public static final String KAFKA_MOVE_TO_EARLIEST_OFFSET = "kafka.move.to.earliest.offset";,
+    public static final String KAFKA_CLIENT_BUFFER_SIZE = "kafka.client.buffer.size";,
+    public static final String KAFKA_CLIENT_SO_TIMEOUT = "kafka.client.so.timeout";,
+    public static final String KAFKA_MAX_PULL_HRS = "kafka.max.pull.hrs";,
+    public static final String KAFKA_MAX_PULL_MINUTES_PER_TASK = "kafka.max.pull.minutes.per.task";,
+    public static final String KAFKA_MAX_HISTORICAL_DAYS = "kafka.max.historical.days";,
+    public static final String CAMUS_MESSAGE_DECODER_CLASS = "camus.message.decoder.class";,
+    public static final String ETL_IGNORE_SCHEMA_ERRORS = "etl.ignore.schema.errors";,
+    public static final String ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST = "etl.audit.ignore.service.topic.list";,
+    public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";,
+    public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";,
+    public static final int FETCH_FROM_LEADER_MAX_RETRIES = 3;,
+    public static final int NUM_TRIES_TOPIC_METADATA = 3;,
+    public static boolean reportJobFailureDueToSkippedMsg = false;,
+    private static Logger log = null;,
+
+    public EtlInputFormat() {
     if (log == null)
       log = Logger.getLogger(getClass());
   }
 
-  public static void setLogger(Logger log) {
+    public static void setLogger(Logger log) {
     EtlInputFormat.log = log;
   }
 
-  @Override
+    @Override
   public RecordReader<EtlKey, CamusWrapper> createRecordReader(InputSplit split, TaskAttemptContext context)
       throws IOException, InterruptedException {
     return new EtlRecordReader(this, split, context);
   }
 
-  /**
+    /**
    * Gets the metadata from Kafka
    *
    * @param context
    * @return
    */
-  public List<TopicMetadata> getKafkaMetadata(JobContext context) {
+
+    /**
+   * Gets the metadata from Kafka
+   * 
+   * @param context
+   * @return
+   */
+
+    public List<TopicMetadata> getKafkaMetadata(JobContext context) {
     ArrayList<String> metaRequestTopics = new ArrayList<String>();
     CamusJob.startTiming("kafkaSetupTime");
     String brokerString = CamusJob.getKafkaBrokers(context);
@@ -155,14 +145,14 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return topicMetadataList;
   }
 
-  private SimpleConsumer createBrokerConsumer(JobContext context, String broker) {
+    private SimpleConsumer createBrokerConsumer(JobContext context, String broker) {
     if (!broker.matches(".+:\\d+"))
       throw new InvalidParameterException("The kakfa broker " + broker + " must follow address:port pattern");
     String[] hostPort = broker.split(":");
     return createSimpleConsumer(context, hostPort[0], Integer.valueOf(hostPort[1]));
   }
 
-  public SimpleConsumer createSimpleConsumer(JobContext context, String host, int port) {
+    public SimpleConsumer createSimpleConsumer(JobContext context, String host, int port) {
     SimpleConsumer consumer =
         new SimpleConsumer(host, port,
             CamusJob.getKafkaTimeoutValue(context), CamusJob.getKafkaBufferSize(context),
@@ -170,14 +160,15 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return consumer;
   }
 
-  /**
+    /**
    * Gets the latest offsets and create the requests as needed
-   *
+   * 
    * @param context
    * @param offsetRequestInfo
    * @return
    */
-  public ArrayList<CamusRequest> fetchLatestOffsetAndCreateEtlRequests(JobContext context,
+
+    public ArrayList<CamusRequest> fetchLatestOffsetAndCreateEtlRequests(JobContext context,
       HashMap<LeaderInfo, ArrayList<TopicAndPartition>> offsetRequestInfo) {
     ArrayList<CamusRequest> finalRequests = new ArrayList<CamusRequest>();
     for (LeaderInfo leader : offsetRequestInfo.keySet()) {
@@ -227,7 +218,16 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return finalRequests;
   }
 
-  private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer,
+
+    /**
+   * Gets the latest offsets and create the requests as needed
+   *
+   * @param context
+   * @param offsetRequestInfo
+   * @return
+   */
+
+    private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer,
       Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, JobContext context) {
     for (int i = 0; i <= FETCH_FROM_LEADER_MAX_RETRIES; i++) {
       try {
@@ -255,7 +255,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return null;
   }
 
-  private String generateLogWarnForSkippedTopics(Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, SimpleConsumer consumer) {
+    private String generateLogWarnForSkippedTopics(Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, SimpleConsumer consumer) {
     StringBuilder sb = new StringBuilder();
     sb.append("The following topics will be skipped due to failure in fetching latest offsets from leader "
         + consumer.host() + ":" + consumer.port());
@@ -265,7 +265,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return sb.toString();
   }
 
-  public String createTopicRegEx(HashSet<String> topicsSet) {
+    public String createTopicRegEx(HashSet<String> topicsSet) {
     String regex = "";
     StringBuilder stringbuilder = new StringBuilder();
     for (String whiteList : topicsSet) {
@@ -277,7 +277,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return regex;
   }
 
-  public List<TopicMetadata> filterWhitelistTopics(List<TopicMetadata> topicMetadataList,
+    public List<TopicMetadata> filterWhitelistTopics(List<TopicMetadata> topicMetadataList,
       HashSet<String> whiteListTopics) {
     ArrayList<TopicMetadata> filteredTopics = new ArrayList<TopicMetadata>();
     String regex = createTopicRegEx(whiteListTopics);
@@ -291,7 +291,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return filteredTopics;
   }
 
-  @Override
+    @Override
   public List<InputSplit> getSplits(JobContext context) throws IOException, InterruptedException {
     CamusJob.startTiming("getSplits");
     ArrayList<CamusRequest> finalRequests;
@@ -438,7 +438,13 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return allocator.allocateWork(finalRequests, context);
   }
 
-  private Set<String> getMoveToLatestTopicsSet(JobContext context) {
+
+    @Override
+      public int compare(CamusRequest r1, CamusRequest r2) {
+        return r1.getTopic().compareTo(r2.getTopic());
+      }
+
+    private Set<String> getMoveToLatestTopicsSet(JobContext context) {
     Set<String> topics = new HashSet<String>();
 
     String[] arr = getMoveToLatestTopics(context);
@@ -452,7 +458,26 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return topics;
   }
 
-  private boolean createMessageDecoder(JobContext context, String topic) {
+    private void writeRequests(List<CamusRequest> requests, JobContext context) throws IOException {
+    FileSystem fs = FileSystem.get(context.getConfiguration());
+    Path output = FileOutputFormat.getOutputPath(context);
+
+    if (fs.exists(output)) {
+      fs.mkdirs(output);
+    }
+
+    output = new Path(output, EtlMultiOutputFormat.REQUESTS_FILE);
+    SequenceFile.Writer writer =
+        SequenceFile.createWriter(fs, context.getConfiguration(), output, EtlRequest.class, NullWritable.class);
+
+    for (CamusRequest r : requests) {
+      //TODO: factor out kafka specific request functionality
+      writer.append(r, NullWritable.get());
+    }
+    writer.close();
+  }
+
+    private boolean createMessageDecoder(JobContext context, String topic) {
     try {
       MessageDecoderFactory.createMessageDecoder(context, topic);
       return true;
@@ -462,7 +487,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     }
   }
 
-  private void writePrevious(Collection<EtlKey> missedKeys, JobContext context) throws IOException {
+    private void writePrevious(Collection<EtlKey> missedKeys, JobContext context) throws IOException {
     FileSystem fs = FileSystem.get(context.getConfiguration());
     Path output = FileOutputFormat.getOutputPath(context);
 
@@ -481,26 +506,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     writer.close();
   }
 
-  private void writeRequests(List<CamusRequest> requests, JobContext context) throws IOException {
-    FileSystem fs = FileSystem.get(context.getConfiguration());
-    Path output = FileOutputFormat.getOutputPath(context);
-
-    if (fs.exists(output)) {
-      fs.mkdirs(output);
-    }
-
-    output = new Path(output, EtlMultiOutputFormat.REQUESTS_FILE);
-    SequenceFile.Writer writer =
-        SequenceFile.createWriter(fs, context.getConfiguration(), output, EtlRequest.class, NullWritable.class);
-
-    for (CamusRequest r : requests) {
-      //TODO: factor out kafka specific request functionality
-      writer.append(r, NullWritable.get());
-    }
-    writer.close();
-  }
-
-  private Map<CamusRequest, EtlKey> getPreviousOffsets(Path[] inputs, JobContext context) throws IOException {
+    private Map<CamusRequest, EtlKey> getPreviousOffsets(Path[] inputs, JobContext context) throws IOException {
     Map<CamusRequest, EtlKey> offsetKeysMap = new HashMap<CamusRequest, EtlKey>();
     for (Path input : inputs) {
       FileSystem fs = input.getFileSystem(context.getConfiguration());
@@ -528,11 +534,11 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return offsetKeysMap;
   }
 
-  public static void setWorkAllocator(JobContext job, Class<WorkAllocator> val) {
+    public static void setWorkAllocator(JobContext job, Class<WorkAllocator> val) {
     job.getConfiguration().setClass(CAMUS_WORK_ALLOCATOR_CLASS, val, WorkAllocator.class);
   }
 
-  public static WorkAllocator getWorkAllocator(JobContext job) {
+    public static WorkAllocator getWorkAllocator(JobContext job) {
     try {
       return (WorkAllocator) job.getConfiguration()
           .getClass(CAMUS_WORK_ALLOCATOR_CLASS, Class.forName(CAMUS_WORK_ALLOCATOR_DEFAULT)).newInstance();
@@ -541,124 +547,126 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     }
   }
 
-  public static void setMoveToLatestTopics(JobContext job, String val) {
+    public static void setMoveToLatestTopics(JobContext job, String val) {
     job.getConfiguration().set(KAFKA_MOVE_TO_LAST_OFFSET_LIST, val);
   }
 
-  public static String[] getMoveToLatestTopics(JobContext job) {
+    public static String[] getMoveToLatestTopics(JobContext job) {
     return job.getConfiguration().getStrings(KAFKA_MOVE_TO_LAST_OFFSET_LIST);
   }
 
-  public static void setKafkaClientBufferSize(JobContext job, int val) {
+    public static void setKafkaClientBufferSize(JobContext job, int val) {
     job.getConfiguration().setInt(KAFKA_CLIENT_BUFFER_SIZE, val);
   }
 
-  public static int getKafkaClientBufferSize(JobContext job) {
+    public static int getKafkaClientBufferSize(JobContext job) {
     return job.getConfiguration().getInt(KAFKA_CLIENT_BUFFER_SIZE, 2 * 1024 * 1024);
   }
 
-  public static void setKafkaClientTimeout(JobContext job, int val) {
+    public static void setKafkaClientTimeout(JobContext job, int val) {
     job.getConfiguration().setInt(KAFKA_CLIENT_SO_TIMEOUT, val);
   }
 
-  public static int getKafkaClientTimeout(JobContext job) {
-    return job.getConfiguration().getInt(KAFKA_CLIENT_SO_TIMEOUT, 60000);
-  }
-
-  public static void setKafkaMaxPullHrs(JobContext job, int val) {
-    job.getConfiguration().setInt(KAFKA_MAX_PULL_HRS, val);
+    public static String[] getKafkaBlacklistTopic(JobContext job) {
+    return getKafkaBlacklistTopic(job.getConfiguration());
   }
 
-  public static int getKafkaMaxPullHrs(JobContext job) {
-    return job.getConfiguration().getInt(KAFKA_MAX_PULL_HRS, -1);
+    public static int getKafkaClientTimeout(JobContext job) {
+    return job.getConfiguration().getInt(KAFKA_CLIENT_SO_TIMEOUT, 60000);
   }
 
-  public static void setKafkaMaxPullMinutesPerTask(JobContext job, int val) {
-    job.getConfiguration().setInt(KAFKA_MAX_PULL_MINUTES_PER_TASK, val);
+    public static String[] getKafkaBlacklistTopic(Configuration conf) {
+    final String blacklistStr = conf.get(KAFKA_BLACKLIST_TOPIC);
+    if (blacklistStr != null && !blacklistStr.isEmpty()) {
+      return conf.getStrings(KAFKA_BLACKLIST_TOPIC);
+    } else {
+      return new String[] {};
+    }
   }
 
-  public static int getKafkaMaxPullMinutesPerTask(JobContext job) {
-    return job.getConfiguration().getInt(KAFKA_MAX_PULL_MINUTES_PER_TASK, -1);
+    public static void setKafkaMaxPullHrs(JobContext job, int val) {
+    job.getConfiguration().setInt(KAFKA_MAX_PULL_HRS, val);
   }
 
-  public static void setKafkaMaxHistoricalDays(JobContext job, int val) {
-    job.getConfiguration().setInt(KAFKA_MAX_HISTORICAL_DAYS, val);
+    public static int getKafkaMaxPullHrs(JobContext job) {
+    return job.getConfiguration().getInt(KAFKA_MAX_PULL_HRS, -1);
   }
 
-  public static int getKafkaMaxHistoricalDays(JobContext job) {
-    return job.getConfiguration().getInt(KAFKA_MAX_HISTORICAL_DAYS, -1);
+    public static void setKafkaMaxPullMinutesPerTask(JobContext job, int val) {
+    job.getConfiguration().setInt(KAFKA_MAX_PULL_MINUTES_PER_TASK, val);
   }
 
-  public static void setKafkaBlacklistTopic(JobContext job, String val) {
-    job.getConfiguration().set(KAFKA_BLACKLIST_TOPIC, val);
+    public static String[] getKafkaWhitelistTopic(JobContext job) {
+    return getKafkaWhitelistTopic(job.getConfiguration());
   }
 
-  public static String[] getKafkaBlacklistTopic(JobContext job) {
-    return getKafkaBlacklistTopic(job.getConfiguration());
+    public static int getKafkaMaxPullMinutesPerTask(JobContext job) {
+    return job.getConfiguration().getInt(KAFKA_MAX_PULL_MINUTES_PER_TASK, -1);
   }
 
-  public static String[] getKafkaBlacklistTopic(Configuration conf) {
-    final String blacklistStr = conf.get(KAFKA_BLACKLIST_TOPIC);
-    if (blacklistStr != null && !blacklistStr.isEmpty()) {
-      return conf.getStrings(KAFKA_BLACKLIST_TOPIC);
+    public static String[] getKafkaWhitelistTopic(Configuration conf) {
+    final String whitelistStr = conf.get(KAFKA_WHITELIST_TOPIC);
+    if (whitelistStr != null&& !whitelistStr.isEmpty()) {
+      return conf.getStrings(KAFKA_WHITELIST_TOPIC);
     } else {
       return new String[] {};
     }
   }
 
-  public static void setKafkaWhitelistTopic(JobContext job, String val) {
-    job.getConfiguration().set(KAFKA_WHITELIST_TOPIC, val);
+    public static void setKafkaMaxHistoricalDays(JobContext job, int val) {
+    job.getConfiguration().setInt(KAFKA_MAX_HISTORICAL_DAYS, val);
   }
 
-  public static String[] getKafkaWhitelistTopic(JobContext job) {
-    return getKafkaWhitelistTopic(job.getConfiguration());
+    public static int getKafkaMaxHistoricalDays(JobContext job) {
+    return job.getConfiguration().getInt(KAFKA_MAX_HISTORICAL_DAYS, -1);
   }
 
-  public static String[] getKafkaWhitelistTopic(Configuration conf) {
-    final String whitelistStr = conf.get(KAFKA_WHITELIST_TOPIC);
-    if (whitelistStr != null&& !whitelistStr.isEmpty()) {
-      return conf.getStrings(KAFKA_WHITELIST_TOPIC);
-    } else {
-      return new String[] {};
-    }
+    public static void setKafkaBlacklistTopic(JobContext job, String val) {
+    job.getConfiguration().set(KAFKA_BLACKLIST_TOPIC, val);
   }
 
-  public static void setEtlIgnoreSchemaErrors(JobContext job, boolean val) {
+    public static void setKafkaWhitelistTopic(JobContext job, String val) {
+    job.getConfiguration().set(KAFKA_WHITELIST_TOPIC, val);
+  }
+
+    public static void setEtlIgnoreSchemaErrors(JobContext job, boolean val) {
     job.getConfiguration().setBoolean(ETL_IGNORE_SCHEMA_ERRORS, val);
   }
 
-  public static boolean getEtlIgnoreSchemaErrors(JobContext job) {
+    public static boolean getEtlIgnoreSchemaErrors(JobContext job) {
     return job.getConfiguration().getBoolean(ETL_IGNORE_SCHEMA_ERRORS, false);
   }
 
-  public static void setEtlAuditIgnoreServiceTopicList(JobContext job, String topics) {
+    public static void setEtlAuditIgnoreServiceTopicList(JobContext job, String topics) {
     job.getConfiguration().set(ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST, topics);
   }
 
-  public static String[] getEtlAuditIgnoreServiceTopicList(JobContext job) {
+    public static String[] getEtlAuditIgnoreServiceTopicList(JobContext job) {
     return job.getConfiguration().getStrings(ETL_AUDIT_IGNORE_SERVICE_TOPIC_LIST, "");
   }
 
-  public static void setMessageDecoderClass(JobContext job, Class<MessageDecoder> cls) {
+    public static void setMessageDecoderClass(JobContext job, Class<MessageDecoder> cls) {
     job.getConfiguration().setClass(CAMUS_MESSAGE_DECODER_CLASS, cls, MessageDecoder.class);
   }
 
-  public static Class<MessageDecoder> getMessageDecoderClass(JobContext job) {
+    public static Class<MessageDecoder> getMessageDecoderClass(JobContext job) {
     return (Class<MessageDecoder>) job.getConfiguration().getClass(CAMUS_MESSAGE_DECODER_CLASS,
         KafkaAvroMessageDecoder.class);
   }
 
-  public static Class<MessageDecoder> getMessageDecoderClass(JobContext job, String topicName) {
+    public static Class<MessageDecoder> getMessageDecoderClass(JobContext job, String topicName) {
     Class<MessageDecoder> topicDecoder = (Class<MessageDecoder>) job.getConfiguration().getClass(
             CAMUS_MESSAGE_DECODER_CLASS + "." + topicName, null);
     return topicDecoder == null ? getMessageDecoderClass(job) : topicDecoder;
   }
 
-  private class OffsetFileFilter implements PathFilter {
+    private class OffsetFileFilter implements PathFilter{
 
-    @Override
+
+        @Override
     public boolean accept(Path arg0) {
       return arg0.getName().startsWith(EtlMultiOutputFormat.OFFSET_PREFIX);
     }
-  }
-}
+
+    }
+}
\ No newline at end of file
