diff --git a/java_case_studies/reference_repos/camus/conflict3/left.java b/java_case_studies/reference_repos/camus/conflict3/right.java
index abd5427..e28308f 100644
--- a/java_case_studies/reference_repos/camus/conflict3/left.java
+++ b/java_case_studies/reference_repos/camus/conflict3/right.java
@@ -1,16 +1,5 @@
 package com.linkedin.camus.etl.kafka.mapred;
 
-import com.linkedin.camus.coders.CamusWrapper;
-import com.linkedin.camus.coders.MessageDecoder;
-import com.linkedin.camus.etl.kafka.CamusJob;
-import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
-import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
-import com.linkedin.camus.etl.kafka.common.EtlKey;
-import com.linkedin.camus.etl.kafka.common.EtlRequest;
-import com.linkedin.camus.etl.kafka.common.LeaderInfo;
-import com.linkedin.camus.workallocater.CamusRequest;
-import com.linkedin.camus.workallocater.WorkAllocator;
-
 import java.io.IOException;
 import java.net.URI;
 import java.security.InvalidParameterException;
@@ -25,7 +14,6 @@ import java.util.List;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Pattern;
 
 import kafka.api.PartitionOffsetRequestInfo;
@@ -38,6 +26,7 @@ import kafka.javaapi.TopicMetadata;
 import kafka.javaapi.TopicMetadataRequest;
 import kafka.javaapi.consumer.SimpleConsumer;
 
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
@@ -53,6 +42,17 @@ import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.log4j.Logger;
 
+import com.linkedin.camus.coders.CamusWrapper;
+import com.linkedin.camus.coders.MessageDecoder;
+import com.linkedin.camus.etl.kafka.CamusJob;
+import com.linkedin.camus.etl.kafka.coders.KafkaAvroMessageDecoder;
+import com.linkedin.camus.etl.kafka.coders.MessageDecoderFactory;
+import com.linkedin.camus.etl.kafka.common.EtlKey;
+import com.linkedin.camus.etl.kafka.common.EtlRequest;
+import com.linkedin.camus.etl.kafka.common.LeaderInfo;
+import com.linkedin.camus.workallocater.CamusRequest;
+import com.linkedin.camus.workallocater.WorkAllocator;
+
 
 /**
  * Input format for a Kafka pull job.
@@ -79,10 +79,8 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   public static final String CAMUS_WORK_ALLOCATOR_CLASS = "camus.work.allocator.class";
   public static final String CAMUS_WORK_ALLOCATOR_DEFAULT = "com.linkedin.camus.workallocater.BaseAllocator";
 
-  public static final int FETCH_FROM_LEADER_MAX_RETRIES = 3;
-
-  public static boolean reportJobFailureDueToSkippedMsg = false;
-
+  public static final int NUM_TRIES_TOPIC_METADATA = 3;
+  
   private static Logger log = null;
 
   public EtlInputFormat() {
@@ -102,7 +100,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
 
   /**
    * Gets the metadata from Kafka
-   * 
+   *
    * @param context
    * @return
    */
@@ -123,13 +121,23 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
       log.info(String.format("Fetching metadata from broker %s with client id %s for %d topic(s) %s", brokers.get(i),
           consumer.clientId(), metaRequestTopics.size(), metaRequestTopics));
       try {
-        topicMetadataList = consumer.send(new TopicMetadataRequest(metaRequestTopics)).topicsMetadata();
-        fetchMetaDataSucceeded = true;
-      } catch (Exception e) {
-        savedException = e;
-        log.warn(
-            String.format("Fetching topic metadata with client id %s for topics [%s] from broker [%s] failed",
-                consumer.clientId(), metaRequestTopics, brokers.get(i)), e);
+        for (int iter = 0; iter < NUM_TRIES_TOPIC_METADATA; iter++) {
+          try {
+            topicMetadataList = consumer.send(new TopicMetadataRequest(metaRequestTopics)).topicsMetadata();
+            fetchMetaDataSucceeded = true;
+            break;
+          } catch (Exception e) {
+            savedException = e;
+            log.warn(
+                     String.format("Fetching topic metadata with client id %s for topics [%s] from broker [%s] failed, iter[%s]",
+                                   consumer.clientId(), metaRequestTopics, brokers.get(i), iter), e);
+            try {
+              Thread.sleep((long)(Math.random() * (iter + 1) * 1000));
+            } catch (InterruptedException ex) {
+              log.warn("Caught InterruptedException: " + ex);
+            }
+          }
+        }
       } finally {
         consumer.close();
         i++;
@@ -156,10 +164,10 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
             CamusJob.getKafkaClientName(context));
     return consumer;
   }
-  
+
   /**
    * Gets the latest offsets and create the requests as needed
-   * 
+   *
    * @param context
    * @param offsetRequestInfo
    * @return
@@ -185,24 +193,19 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         earliestOffsetInfo.put(topicAndPartition, partitionEarliestOffsetRequestInfo);
       }
 
-      OffsetResponse latestOffsetResponse = getLatestOffsetResponse(consumer, latestOffsetInfo, context);
-      OffsetResponse earliestOffsetResponse = null;
-      if (latestOffsetResponse != null) {
-        earliestOffsetResponse = getLatestOffsetResponse(consumer, earliestOffsetInfo, context);
-      }
+      OffsetResponse latestOffsetResponse =
+          consumer.getOffsetsBefore(new OffsetRequest(latestOffsetInfo, kafka.api.OffsetRequest.CurrentVersion(),
+              CamusJob.getKafkaClientName(context)));
+      OffsetResponse earliestOffsetResponse =
+          consumer.getOffsetsBefore(new OffsetRequest(earliestOffsetInfo, kafka.api.OffsetRequest.CurrentVersion(),
+              CamusJob.getKafkaClientName(context)));
       consumer.close();
-      if (earliestOffsetResponse == null) {
-        log.warn(generateLogWarnForSkippedTopics(earliestOffsetInfo, consumer));
-        reportJobFailureDueToSkippedMsg = true;
-        continue;
-      }
-
       for (TopicAndPartition topicAndPartition : topicAndPartitions) {
         long latestOffset = latestOffsetResponse.offsets(topicAndPartition.topic(), topicAndPartition.partition())[0];
         long earliestOffset =
             earliestOffsetResponse.offsets(topicAndPartition.topic(), topicAndPartition.partition())[0];
 
-        //TODO: factor out kafka specific request functionality 
+        //TODO: factor out kafka specific request functionality
         CamusRequest etlRequest =
             new EtlRequest(context, topicAndPartition.topic(), Integer.toString(leader.getLeaderId()),
                 topicAndPartition.partition(), leader.getUri());
@@ -214,44 +217,6 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     return finalRequests;
   }
 
-  private OffsetResponse getLatestOffsetResponse(SimpleConsumer consumer,
-      Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, JobContext context) {
-    for (int i = 0; i <= FETCH_FROM_LEADER_MAX_RETRIES; i++) {
-      try {
-        OffsetResponse offsetResponse = consumer.getOffsetsBefore(new OffsetRequest(offsetInfo,
-            kafka.api.OffsetRequest.CurrentVersion(), CamusJob.getKafkaClientName(context)));
-        if (offsetResponse.hasError()) {
-          throw new RuntimeException("offsetReponse has error.");
-        }
-        return offsetResponse;
-      } catch (Exception e) {
-        log.warn("Fetching offset from leader " + consumer.host() + ":" + consumer.port()
-            + " has failed " + (i + 1) + " time(s). Reason: "
-            + e.getMessage() + " "
-            + (FETCH_FROM_LEADER_MAX_RETRIES - i) + " retries left.");
-        if (i < FETCH_FROM_LEADER_MAX_RETRIES) {
-          try {
-            Thread.sleep(1000 * (i + 1));
-          } catch (InterruptedException e1) {
-            log.error("Caught interrupted exception between retries of getting latest offsets. "
-                + e1.getMessage());
-          }
-        }
-      }
-    }
-    return null;
-  }
-
-  private String generateLogWarnForSkippedTopics(Map<TopicAndPartition, PartitionOffsetRequestInfo> offsetInfo, SimpleConsumer consumer) {
-    StringBuilder sb = new StringBuilder();
-    sb.append("The following topics will be skipped due to failure in fetching latest offsets from leader "
-        + consumer.host() + ":" + consumer.port());
-    for (TopicAndPartition topicAndPartition : offsetInfo.keySet()) {
-      sb.append("  " + topicAndPartition.topic());
-    }
-    return sb.toString();
-  }
-
   public String createTopicRegEx(HashSet<String> topicsSet) {
     String regex = "";
     StringBuilder stringbuilder = new StringBuilder();
@@ -349,6 +314,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     finalRequests = fetchLatestOffsetAndCreateEtlRequests(context, offsetRequestInfo);
 
     Collections.sort(finalRequests, new Comparator<CamusRequest>() {
+      @Override
       public int compare(CamusRequest r1, CamusRequest r2) {
         return r1.getTopic().compareTo(r2.getTopic());
       }
@@ -361,7 +327,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
     for (CamusRequest request : finalRequests) {
       if (moveLatest.contains(request.getTopic()) || moveLatest.contains("all")) {
         log.info("Moving to latest for topic: " + request.getTopic());
-        //TODO: factor out kafka specific request functionality 
+        //TODO: factor out kafka specific request functionality
         EtlKey oldKey = offsetKeys.get(request);
         EtlKey newKey =
             new EtlKey(request.getTopic(), ((EtlRequest) request).getLeaderId(), request.getPartition(), 0,
@@ -396,7 +362,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
           request.setOffset(request.getEarliestOffset());
           offsetKeys.put(
               request,
-              //TODO: factor out kafka specific request functionality 
+              //TODO: factor out kafka specific request functionality
               new EtlKey(request.getTopic(), ((EtlRequest) request).getLeaderId(), request.getPartition(), 0, request
                   .getOffset()));
         } else {
@@ -480,8 +446,8 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         SequenceFile.createWriter(fs, context.getConfiguration(), output, EtlRequest.class, NullWritable.class);
 
     for (CamusRequest r : requests) {
-      //TODO: factor out kafka specific request functionality 
-      writer.append((EtlRequest) r, NullWritable.get());
+      //TODO: factor out kafka specific request functionality
+      writer.append(r, NullWritable.get());
     }
     writer.close();
   }
@@ -495,7 +461,7 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
         SequenceFile.Reader reader = new SequenceFile.Reader(fs, f.getPath(), context.getConfiguration());
         EtlKey key = new EtlKey();
         while (reader.next(key, NullWritable.get())) {
-          //TODO: factor out kafka specific request functionality 
+          //TODO: factor out kafka specific request functionality
           CamusRequest request = new EtlRequest(context, key.getTopic(), key.getLeaderId(), key.getPartition());
           if (offsetKeysMap.containsKey(request)) {
 
@@ -580,9 +546,13 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   }
 
   public static String[] getKafkaBlacklistTopic(JobContext job) {
-    if (job.getConfiguration().get(KAFKA_BLACKLIST_TOPIC) != null
-        && !job.getConfiguration().get(KAFKA_BLACKLIST_TOPIC).isEmpty()) {
-      return job.getConfiguration().getStrings(KAFKA_BLACKLIST_TOPIC);
+    return getKafkaBlacklistTopic(job.getConfiguration());
+  }
+
+  public static String[] getKafkaBlacklistTopic(Configuration conf) {
+    final String blacklistStr = conf.get(KAFKA_BLACKLIST_TOPIC);
+    if (blacklistStr != null && !blacklistStr.isEmpty()) {
+      return conf.getStrings(KAFKA_BLACKLIST_TOPIC);
     } else {
       return new String[] {};
     }
@@ -593,9 +563,13 @@ public class EtlInputFormat extends InputFormat<EtlKey, CamusWrapper> {
   }
 
   public static String[] getKafkaWhitelistTopic(JobContext job) {
-    if (job.getConfiguration().get(KAFKA_WHITELIST_TOPIC) != null
-        && !job.getConfiguration().get(KAFKA_WHITELIST_TOPIC).isEmpty()) {
-      return job.getConfiguration().getStrings(KAFKA_WHITELIST_TOPIC);
+    return getKafkaWhitelistTopic(job.getConfiguration());
+  }
+
+  public static String[] getKafkaWhitelistTopic(Configuration conf) {
+    final String whitelistStr = conf.get(KAFKA_WHITELIST_TOPIC);
+    if (whitelistStr != null&& !whitelistStr.isEmpty()) {
+      return conf.getStrings(KAFKA_WHITELIST_TOPIC);
     } else {
       return new String[] {};
     }
